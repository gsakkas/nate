\section{Learning to Blame}
\label{sec:learning}
Next, we describe our approach to localizing type errors, in the context
of \lang (\autoref{fig:syntax}), a simple lambda calculus with integers,
booleans, pairs, and lists.
%
\input{syntax}
%
First, we define the inputs to the model, a set of \emph{features}
(\autoref{sec:features}) that we will use to describe programs.
%
Second, we define the expected outputs of the model, a set of
\emph{labels} (\autoref{sec:labels}) that we will use to assign blame.
%
Third, we define the actual \emph{models} (\autoref{sec:models}) that we
will evaluate.

\subsection{Features}
\label{sec:features}
The first issue we must tackle is formulating our learning task in
machine learning terms.
%
We are given programs over $e$, but the learning algorithms expect to work
with \emph{feature vectors} --- vectors of real numbers, where each
column describes a particular aspect of the input.
%
Thus, our first task is to convert programs to feature vectors.

We choose to model a program as a \emph{set} of feature vectors, where
each element corresponds an expression in the program. We group the
features into the following categories.

\paragraph{Local syntactic features}
These features describe the syntactic category of each expression $e$.
%
In other words, for each production of $e$ in \autoref{fig:syntax} we
introduce a feature that is enabled (set to $1$) \emph{iff} the
expression was built with that production.

\paragraph{Contextual syntactic features}
These are like local syntactic features, but lifted to describe the
parent and children of the current expression.
%
If a particular $e$ does not have children (\eg a variable $x$) or a
parent (\ie the root expression), we leave the corresponding features
disabled (set to $0$).
%
This gives us a notion of the \emph{context} in which an expression
occurs, similar to the \emph{n-grams} commonly found in linguistic
models.

Instead of just describing the immediate context, we could describe
whether a particular syntax element occurs in the neighboring
expressions (or even a count of how many times it occurs).
%
Such fuzzier notions of context may enable increased precision in the
model, but they also introduce opportunities for \emph{overfitting} ---
where the model memorizes particular inputs rather than learning general
patterns.
%
We will investigate (\ES{maybe..}) the impact of these alternatives
in \autoref{sec:evaluation}.

\paragraph{Expression size}
We also add a feature representing the \emph{size} of each expression,
\ie how many sub-expressions does it contain?
%
This allows the model to learn that, \eg, expressions closer to the
leaves are more likely to be blamed than expressions closer to the root.

\paragraph{Typing features}
A natural way of summarizing the context in which an expression occurs
is with \emph{types}.
%
A difficulty that arises here is that, due to the parametric type
constructors $\tfun{\cdot}{\cdot}$, $\tprod{\cdot}{\cdot}$, and
$\tlist{\cdot}$, there is an \emph{infinite} set of possible types ---
but we must have a \emph{finite} set of features.
%
Thus, we abstract the type of an expression to the set of type
constructors it \emph{mentions}, and add features for each type
constructor that describe whether a given type mentions the type
constructor.
%
For example, the type $\tint$ would only enable the $\tint$ feature,
while the type $\tfun{\tint}{\tbool}$ would enable the
$\tfun{\cdot}{\cdot}$, $\tint$, and $\tbool$ features.
%
We add these features for parent and child expressions to summarize the
context, but also for the current expression, as the type of an
expression is not always clear \emph{syntactically} (\eg for variables
and applications).
%
\ES{note the possibility of traversal bias from typing features}

\paragraph{Type error slice}
Finally, we would like the model to be able to distinguish between
changes that could fix the error, and changes that
\emph{cannot possibly} fix the error.
%
Thus, we compute a minimal type error \emph{slice} for the program
(\ie the set of expressions that contribute to the error), and add a
feature that is enabled \emph{iff} an expression is part of the slice.
%
If the program contains multiple type errors, we will compute a minimal
slice for each error.

\subsection{Labels}
\label{sec:labels}
We define the outputs of the model to be a simple ``good'' or ``bad''
label, where ``good'' means the expression \emph{should not} change and
``bad'' means the expression \emph{should} change.
%
This allows us to predict whether any individual expression should
change, but we would actually like to predict the \emph{most likely}
expressions to change.
%
Many learning algorithms produce not only a prediction, but also a
metric that can be interpreted as a \emph{confidence} in the prediction.
%
Thus, we add a post-processing step where we \emph{rank} the expressions
by the model's confidence that they will change, and select only the top
$k$ (in practice $k=3$) to present to the user.

\subsection{Models}
\label{sec:models}





%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
