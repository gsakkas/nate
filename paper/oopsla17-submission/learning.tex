\section{Learning to Blame}
\label{sec:learning}
In this section, we describe our approach to localizing type errors, in the
context
of \lang (\autoref{fig:syntax}), a simple lambda calculus with integers,
booleans, pairs, and lists.
%
\input{syntax}
%
First, we define the inputs to the model, a set of \emph{features}
(\autoref{sec:features}) that we will use to describe programs.
%
Second, we define the expected outputs of the model, a set of
\emph{labels} (\autoref{sec:labels}) that we will use to assign blame.
%
Third, we describe the actual \emph{models} (\autoref{sec:models}) that we
will evaluate.

\subsection{Features}
\label{sec:features}

% \begin{figure}[ht]
% \begin{minipage}{0.6\linewidth}
% \begin{code}
%   let rec sumList xs =
%     match xs with
%     | []     -> []
%     | hd::tl -> hd + sumList tl
% \end{code}
% \end{minipage}
% \begin{minipage}{0.3\linewidth}
% \begin{code}[numbers=left]
% []
% hd + sumList tl
% sumList tl
% tl
% \end{code}
% \end{minipage}
% \caption{An ill-typed program (left) and a selection of its
%   sub-expressions (right).}
% \label{fig:sumList}
% \end{figure}
\begin{table}[ht]
\caption{Example Feature Vectors}\label{tab:sumList}
\begin{tabular}{lrrrrrr}
\toprule
Expression
  & \IsNil & \IsCaseListP & \ExprSize
  & \HasTypeIntCOne & \HasTypeList & \InSlice \\
\midrule
|[]|
  & 1 & 1 & 1 & 0 & 1 & 1 \\
|hd + sumList tl|
  & 0 & 1 & 5 & 1 & 0 & 1 \\
|sumList tl|
  & 0 & 0 & 3 & 0 & 1 & 1 \\
|tl|
  & 0 & 0 & 1 & 0 & 1 & 0 \\
\bottomrule
\end{tabular}
\bigskip
\caption*{A selection of the features we would extract from the
\lstinline!sumList! program in \autoref{fig:sumList}. A feature is
considered \emph{enabled} if it has a non-zero value, and
\emph{disabled} otherwise. A ``-P'' suffix indicates that the feature
describes the parent of the current expression, a ``-C$n$'' suffix
indicates that the feature describes the $n$-th (left-to-right) child of
the current expression.  Note that, since we rely on a partial typing
derivation, we are subject to the well-known traversal bias and label
the expression \lstinline!sumList tl! as having type
$\tlist{\cdot}$. The model will have to learn to correct for this bias.}
\end{table}
The first issue we must tackle is formulating our learning task in
machine learning terms.
%
We are given programs over \lang, but the learning algorithms expect to work
with \emph{feature vectors} --- vectors of real numbers, where each
column describes a particular aspect of the input.
%
Thus, our first task is to convert programs to feature vectors.

We choose to model a program as a \emph{set} of feature vectors, where
each element corresponds an expression in the program.
%
Thus, given the |sumList| program in \autoref{fig:sumList} we
would first split it into its constituent sub-expressions and then
transform each sub-expression into a single feature vector.
%
We group the features into five categories, using \autoref{tab:sumList}
as a running example of the feature extraction process.

\subsubsection{Local syntactic features}
These features describe the syntactic category of each expression $e$.
%
In other words, for each production of $e$ in \autoref{fig:syntax} we
introduce a feature that is enabled (set to $1$) if the expression was
built with that production, and disabled (set to $0$) otherwise.
%
For example, the \IsNil feature in \autoref{tab:sumList} describes
whether the current expression is the empty list $\enil$.

We distinguish between matching on a list vs.\ on a pair, as this
affects the typing derivation.
%
We also assume that all pattern matches are well-formed --- \ie all
patterns must match on the same type.
%
Ill-formed match expressions would lead to a type error; however, they
are already effectively localized to the match expression itself.
%
We note that this is not a \emph{fundamental} limitation, and one could
easily add features that specify whether a match \emph{contains} a
particular pattern, and thus have a match expression that enables multiple
features.

\subsubsection{Contextual syntactic features}
These are similar to local syntactic features, but lifted to describe the
parent and children of the current expression.
%
For example, the \IsCaseListP feature in \autoref{tab:sumList} describes
whether current expression's \emph{parent} matches on a list.
%
If a particular $e$ does not have children (\eg a variable $x$) or a
parent (\ie the root expression), we leave the corresponding features
disabled.
%
This gives us a notion of the \emph{context} in which an expression
occurs, similar to the \emph{n-grams} commonly used in linguistic
models \citep{Hindle2012-hf,Gotterbarn1998-gc}.

% Instead of just describing the immediate context, we could describe
% whether a particular syntax element occurs in the neighboring
% expressions (or even a count of how many times it occurs).
% %
% For example, the \CountVarP feature in \autoref{tab:sumList} describes
% how many variables are contained in the expression \emph{rooted} at the
% current expression's parent.
% %
% Such fuzzier notions of context may enable increased precision in the
% model, but they also introduce opportunities for \emph{overfitting} ---
% where the model memorizes particular inputs rather than learning general
% patterns.
% %
% We will investigate (\ES{maybe..}) the impact of these alternatives
% in \autoref{sec:evaluation}.

\subsubsection{Expression size}
We also propose a feature representing the \emph{size} of each expression,
\ie how many sub-expressions does it contain?
%
For example, the \ExprSize feature in \autoref{tab:sumList} is set to three
for the expression |sumList tl| as it contains three expressions:
the two variables and the application itself.
%
This allows the model to learn that, \eg, expressions closer to the
leaves are more likely to be blamed than expressions closer to the root.

\subsubsection{Typing features}
A natural way of summarizing the context in which an expression occurs
is with \emph{types}.
%
Of course, the programs we are given are \emph{untypeable}, but we can
still extract a \emph{partial} typing derivation from the type checker
and use it to provide more information to the model.

A difficulty that arises here is that, due to the parametric type
constructors $\tfun{\cdot}{\cdot}$, $\tprod{\cdot}{\cdot}$, and
$\tlist{\cdot}$, there is an \emph{infinite} set of possible types ---
but we must have a \emph{finite} set of features.
%
Thus, we abstract the type of an expression to the set of type
constructors it \emph{mentions}, and add features for each type
constructor that describe whether a given type mentions the type
constructor.
%
For example, the type $\tint$ would only enable the $\tint$ feature,
while the type $\tfun{\tint}{\tbool}$ would enable the
$\tfun{\cdot}{\cdot}$, $\tint$, and $\tbool$ features.

We add these features for parent and child expressions to summarize the
context, but also for the current expression, as the type of an
expression is not always clear \emph{syntactically}.
%
For example, the expressions |tl| and |sumList tl|
in \autoref{tab:sumList} both enable the feature \HasTypeList, as they
are both inferred to have a type that mentions the $\tlist{\cdot}$
constructor.

Note that our use of typing features in an ill-typed program
subjects us to \emph{traversal
bias}~\cite{McAdam1998-ub,Yang2000-yr}. For example, the 
|sumList tl| expression might alternatively be assigned
the type $\tint$.
%
We rely on the model to counteract this bias (see \autoref{sec:evaluation}).

\subsubsection{Type error slice}
Finally, we desire a model that distinguishes between
changes that could fix the error, and changes that
\emph{cannot possibly} fix the error.
%
Thus, we compute a minimal type error \emph{slice} for the program
(\ie the set of expressions that contribute to the error), and add a
feature that is enabled for expressions that are part of the slice.
%
The \InSlice feature in \autoref{tab:sumList} indicates whether an
expression is part of such a minimal slice, and is enabled for all of
the sampled expressions except for |tl|, which does not affect
the type error.
%
If the program contains multiple type errors, we compute
a minimal slice for each error.

\subsection{Labels}
\label{sec:labels}
We define the outputs of the model to be a simple ``good'' or ``bad''
label, where ``good'' means the expression \emph{should not} change and
``bad'' means the expression \emph{should} change. 
%
This allows us to predict whether any individual expression should
change, but we would actually like to predict the \emph{most likely}
expressions to change.
%
Many learning algorithms produce not only a prediction, but also a
metric that can be interpreted as a \emph{confidence} in the prediction.
%
Thus, we add a post-processing step where we \emph{rank} the expressions
by the model's confidence that they will change, and select only the top
$k$ (in practice $k=3$) to present to the user.

\subsection{Learning Algorithms}
\label{sec:models}
\ES{TODO: mention that this is a SUPERVISED, CLASSIFICATION problem}
%
There are many learning algorithms to choose from, existing
on a spectrum that balances expressiveness with ease of training (and of
interpreting the learned model).
%
In this section we present the learning algorithms we will investigate,
beginning with the simplest.
%
For each algorithm, we describe \ES{TODO...}

\paragraph{Logistic regression}
The simplest model we investigate is logistic regression, which learns a
linear function of the features.
%
The goal is to learn a set of weights $W$ and biases $b$ such that the
function
%
$$
y = Wx + b
$$
%
effectively maps feature vectors $x$ to labels $y$.

\ES{TODO: talk about logistic regression and the loss function?}

Linear models, like logistic regression, are popular for their ease of
% FIXME: Are you sure you want to call LOGISTIC regression a LINEAR model? 
training and of interpreting the resulting model; however, they can be
limited in their applicability.
%
As the name implies, linear models may perform poorly when asked to
learn an inherently \emph{nonlinear} function.
%
This limitation can be mitigated to some extent by adding more features
to the model (\eg combinations of the existing features \ES{CITE?}), but
the fundamental issue remains.

\paragraph{Decision trees}
\ES{TODO: Maybe Huma?}

\paragraph{Random forests}
\ES{TODO: Maybe Huma?}

\paragraph{Neural networks}
The last (and most complex) model we discuss is Artificial Neural
Networks (ANN).

The fundamental building block of an ANN is the linear model, the
nonlinearity arises from the \emph{activation functions} that govern
whether an individual linear model propagates its signal to the next layer.





%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
