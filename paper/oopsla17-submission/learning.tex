\section{Learning to Blame}
\label{sec:learning}

\subsection{Syntax}
\label{sec:syntax}
\input{syntax}
%
\autoref{fig:syntax} describes the syntax of \lang, a simple lambda
calculus with integers, booleans, pairs, and lists.

\subsection{Features}
\label{sec:features}
The first issue we must tackle is formulating our learning task in
machine learning terms.
%
We are given expressions $e$, but the learning algorithms expect to work
with \emph{feature vectors} --- vectors of real numbers, where each
column describes a particular feature of the input.
%
Thus, our first task is to convert expressions to feature vectors.

We choose to model a program as a \emph{set} of feature vectors, where
each element corresponds a sub-expression in the program. We group the
features into the following categories.

\paragraph{Local syntactic features}
These features describe the syntactic category of each sub-expression
$e$.
%
In other words, for each production of $e$ in \autoref{fig:syntax} we
introduce a feature that is enabled (set to $1$) \emph{iff} the
sub-expression was built with that production.

\paragraph{Contextual syntactic features}
These are like local syntactic features, but lifted to describe the
parent and children of the current sub-expression.
%
If a particular $e$ does not have children (\eg a variable $x$) or a
parent (\ie the root expression), we leave the corresponding features
disabled (set to $0$).
%
This gives us a notion of the \emph{context} in which an expression
occurs, similar to the \emph{n-grams} commonly found in linguistic
models.

Instead of just describing the immediate context, we could describe
whether a particular syntax element occurs in the neighboring
sub-expressions (or even a count of how many times it occurs).
%
Such fuzzier notions of context may enable increased precision in the
model, but they also introduce opportunities for \emph{overfitting} ---
where the model memorizes particular inputs rather than learning general
patterns.
%
We will investigate (\ES{maybe..}) the impact of these alternatives
in \autoref{sec:evaluation}.

\paragraph{Expression size}
We also add a feature representing the \emph{size} of each expression,
\ie how many sub-expressions does it contain?
%
This allows the model to learn that, \eg, expressions closer to the
leaves are more likely to be blamed than expressions closer to the root.

\paragraph{Typing features}
A natural way of summarizing the context in which an expression occurs
is with \emph{types}.
%
A difficulty that arises here is that, due to the parametric type
constructors $\tfun{\cdot}{\cdot}$, $\tprod{\cdot}{\cdot}$, and
$\tlist{\cdot}$, there is an \emph{infinite} set of possible types ---
but we must have a \emph{finite} set of features.
%
Thus, we add features for each type \emph{constructor} that describe
whether a given type \emph{mentions} the type constructor.
%
For example, the type $\tint$ would only enable the $\tint$ feature,
while the type $\tfun{\tint}{\tbool}$ would enable the
$\tfun{\cdot}{\cdot}$, $\tint$, and $\tbool$ features.
%
We add these features for parent and child expressions to summarize the
context, but also for the current expression, as the type of an
expression is not always clear \emph{syntactically} (\eg for variables
and applications).
\ES{note the possibility of traversal bias from typing features}

\paragraph{Type error slice}
Finally, we would like the model to be able to distinguish between
changes that could fix the error, and changes that
\emph{cannot possibly} fix the error.
%
Thus, we compute a minimal type error \emph{slice} for the program
(\ie the set of expressions that contribute to the error), and add a
feature that is enabled \emph{iff} an expression is part of the slice.
%
If the program contains multiple type errors, we will compute a minimal
slice for each error.

\subsection{Labels}
\label{sec:labels}

\subsection{Models}
\label{sec:models}





%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
