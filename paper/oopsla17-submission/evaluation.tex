\section{Quantitative Evaluation}
\label{sec:evaluation}
\input{data}

We have implemented our technique for localizing type errors for a
purely functional subset of \ocaml with polymorphic types and functions.
%
We seek to answer three questions in our evaluation:
%
\begin{description}
\item[1. Blame Accuracy (\autoref{sec:quantitative})]
  %
  How often do we \emph{correctly} assign blame for the type error?
  %
  % We compare our technique with a variety of off-the-shelf classifiers
  % and find that our top-ranked blame assignments have an accuracy of
  % 72\%, compared to a state-of-the-art 56\%.
  % For how many ill-typed programs can we accurately predict the source
  % of the error?
\item[2. Feature Utility (\autoref{sec:feature-utility})]
  %
  Which feature sets are \emph{most important} for accurate predictions?
  % How much do the features described in \autoref{sec:features}
  % contribute to our predictions?
\item[3. Explaining Predictions (\autoref{sec:qualitative})]
  %
  Can we \emph{explain} individual predictions (correct or not)?
\end{description}
%\ES{may want to swap (1) and (2)..}

\mysubsection{Methodology}
\label{sec:methodology}

We answer our questions on two sets of data gathered from the
undergraduate Programming Languages course at
\begin{anonsuppress}
UC San Diego (IRB \#140608).
\end{anonsuppress}
\begin{noanonsuppress}
AUTHOR's INSTITUTION.
\end{noanonsuppress}
%
We recorded each interaction with the \ocaml top-level system over the
course of the first three assignments, capturing
ill-typed programs and, crucially, their subsequent fixes.
%
The first dataset comes from the Spring 2014 class (\SPRING), with a
cohort of 46 students. The second comes from the Fall 2015 class
(\FALL), with a cohort of 56 students.
%
The extracted programs are relatively small, but they demonstrate a
range of functional programming idioms, \eg higher-order functions and
(polymorphic) algebraic data types.

\mypara{Feature Selection}
We extract a set of 277 features from each sub-expression in a student
program, including:
%
\begin{enumerate}
\item 44 local syntactic features. In addition to the syntax of \lang,
  we support the full range of arithmetic operators (integer and
  floating point), equality and comparison operators, character and
  string literals, and a user-defined |expr| type of simple arithmetic
  expressions. We discuss the challenge of supporting other
  user-defined types in \autoref{sec:discussion}.
\item 176 contextual syntactic features. For each sub-expression we
  additionally extract the local syntactic features of its parent and
  first, second, and third (left-to-right) children. If an expression
  does not have a parent or children, these features will simply be
  disabled. If an expression has more than three children, the
  classifiers will receive no information about the additional
  children.
\item 55 typing features. In addition to the types of \lang, we support
  |int|s, |float|s, |char|s, |string|s, and the user-defined |expr|
  mentioned above. These features are extracted for each sub-expression
  and for the contextual sub-expressions.
\item One feature denoting the size of each sub-expression.
\item One feature denoting whether each sub-expression is part of the
  minimal type error slice. We use this feature as a ``hard''
  constraint, sub-expressions that are not part of the minimal slice
  will be preemptively discarded. We justify this decision in
  \autoref{sec:feature-utility}.
\end{enumerate}

\mypara{Blame Oracle}
Recall from \autoref{sec:labels} that we automatically extract a blame
oracle for each ill-typed program from the (AST) diff between it and the
student's eventual fix.
%
A disadvantage of using diffs in this manner is that students may have
made many, potentially unrelated, changes between compilations; at some
point the ``fix'' becomes a ``rewrite''.
%
We do not wish to consider the ``rewrites'' in our evaluation, so we
discard outliers where the fraction of expressions that have changed is
more than one standard deviation above the mean, establishing a diff
threshold of 44\%.
%
This accounts for roughly 14\% of each dataset, leaving us with
2,425 program pairs for \SPRING and 2,325 pairs for \FALL.

% we discard any program pairs where more than 40\%
% of the sub-expressions have changed.
% %
% We picked 40\% as an estimate of the inflection point where we could
% still retain the large majority of program pairs.
% % FIXME: Can you say that this dataset curation is similar to any other
% % datasets (e.g., the washington one)? Anything you could cite and discuss
% % here would take some of the pressure off.


\mypara{Accuracy Metric}
All of the tools we compare (with the exception of the standard \ocaml
compiler) can produce a list of potential error locations.
%
However, in a study of fault localization techniques,
\citet{Kochhar2016-oc} show that most developers will not consider more
than around five potential error locations before falling back to manual
debugging.
%
Type errors are relatively simple in comparison to general fault
localization, thus we limit our evaluation to the top three predictions
of each tool.
%
We evaluate each tool on whether a changed expression occurred in its
top one, top two, or top three predictions.

\input{evaluation-accuracy}
\input{evaluation-utility}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
