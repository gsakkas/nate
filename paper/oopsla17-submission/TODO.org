20 pages total (weird!!)


* Intro [2p]

** Motivation
   - Type errors are common stumbling block
   - Errors often *reported* far from *source*
   - Existing localization techniques based on hand-written heuristics
     - Occam's Razor
     - Don't account for errors novices *actually make*

** Data-Driven Diagnosis
   - Type error localization based on learning a model of novice mistakes
   - Train classifier to identify error locations

   1. *Data Set*
      Gather a giant data set of error-fixes by ...

   2. *Programs To Tree-Node-Vectors*
      - Model programs and fixes as feature vectors (RJ: CRUCIAL: need cool name for model)
      - Context captured by widening

   3. *Train Classifiers*
      - Train a variety of classifiers on tree-node-vectors
      - When is an AST node likely to be a "FIX"

** Large-Scale Evaluation

   - Evaluate competing techniques on >4500 student programs
   - Judge correctness by student's eventual fix
   - Compare Ocaml, SherrLoc, Mycroft to D3
   - Largest evaluation we know of.

* Overview [4p]

  - START WITH
    - 1, 2 simple examples of programs
    - What people "know", take them into what's new
    - where does OCAML complain
    - where is the REAL error
    - where is the STUDENT fix
    - Set up the problem: can we use student fix to
      choose between "logically equivalent" error
      locations?

  - *Supervised Classification Problem*:
    Goal is to predict a *label* ("good" or "bad")
    Given training set with known-correct labels

  - *Challenge*:
    How to represent programs to classifier?
    Program has rich (variable-size) tree structure, classifier expects
    (fixed-size) "feature vector"
    *Solution*:
    Program ==> *SET* of feature vectors (one per sub-expression)
    Features ==> Predicates on AST
    Features provide all contextual information
    (e.g. "I am a function call and my left child has a function type")

  - *Challenge*:
    How to identify "correct" locations?
    *Solution*:
    Use student's fixes!
    Collect time-series of ill-typed programs and subsequent fixes,
    AST diff to extract changes

  - *Challenge*:
    What to present as user feedback?
    (Probably don't want to show *all* "suspicious" exprs,
    see fault localization study)
    *Solution*:
    Rank predictions by classifier's "confidence",
    select top-k as final predictions

* Learning To Blame [4p]
** Features
*** Local syntactic
*** Contextual syntactic
*** Contextual typing
*** Expression size
*** Type error slice
** Labels
*** Good/Bad
*** Based on AST diff
*** Rank by confidence, select top $k$
** Models
*** Linear
*** Decision Tree
*** Random Forest
*** Multilayer Perceptron
* Evaluation [7p]
** Performance (quantitative)
*** Start with SP14 data
*** "Full" model (~300 features)
**** Linear vs decision vs perceptron
*** Drop samples outside of slice
**** Too important to leave up to the model
**** Also drastically reduces search space
*** Drop contextual syntax features (~100 features)
**** Possible source of overfitting?
**** Remaining features resemble info available to typechecker (+ size)
*** Generalize to FA15
*** Compress perceptron model?
*** Compare to ocaml/sherrloc/mycroft
*** SOMEDAY Generalize to Seminal data?
*** SOMEDAY Impact of adding N-Gram features?
** What did the model learn?
*** Relative importance of features
** Qualitative, i.e. mini-gallery
* Related [3p]
** Type Errors
*** Locating
*** Explaining
*** Fixing
** Fault Localization
** Machine Learning for PL
*** Predicting Program Properties from BigCode
*** ??


* NOTES
** sp14/0219.ml
(3,42)-(3,47): =clone= in =clone ([x] @ acc) (n - 1)=
- incorrectly predict NOCHANGE
- only enabled features on path are =F-Is-Type-Fun= and =F-Is-App-P=
- note that leaf is not clearly in favor of NOCHANGE (only 2 to 1)
  - possibly need more features to identify?

(3,62)-(3,63): =x= in =clone ([x] @ acc) (n - 1)=
- incorrectly predict CHANGE
- only enabled features on path are =F-Is-Type-Var= and =F-Is-List-P=
- VERY confident in prediction, weird!

(3,48)-(3,69): =([x] @ acc)= in =clone ([x] @ acc) (n - 1)=
- incorrectly predict CHANGE
- only enabled features on path are =F-Is-App-P= and =F-Is-Type-List-C3=
- VERY confident, all training samples were true positive, but only ~20 in total
- perhaps need more training data? prediction seems totally bogus...
