\mysection{Conclusion}
\label{sec:conclusion}

We have presented \toolname, which
combines modern statistical methods
with domain-specific feature engineering
to open the door to a new data-driven
path to precise error localization.
%
\toolname
translates pairs of
blame-labeled ill-typed
programs into bags-of-abstracted-terms,
and then trains a classifier
to predict whether a feature vector
should get a blame label.
% that
% predicts the value of the
% whether a feature vector gets
% a blame label.
%
Given a new ill-typed program,
we extract its bag-of-abstracted-terms
to compute the blame label for each
term and rank the predictions based
on the model's confidence, to select
the top 1--3 blame assignments to
present to the user.

We have evaluated our approach with
five different classifiers on a suite
of over 4,500 student programs.
%
We found that while machine learning
over syntactic features of each term in isolation
performs worse than existing
purely constraint based approaches, % (\eg \ocaml, \sherrloc),
augmenting the data with a single feature corresponding to
the type error slice brings our
classifiers up to par with the state of the art,
and further augmenting the data with
features of an expression's parent and children
allows our classifiers to outperform
the state of the art by 18 percentage points.

% We find that while machine
% learning on its own performs slightly worse
% than existing purely constraint based
% approaches (\eg \ocaml, \sherrloc), once
% we augment the data with feature abstraction
% corresponding to the \emph{type error slice}~\cite{Tip2001-qp},
% the resulting models models outperform
% the state of the art by up to 18
% percentage points.


As with other forms of machine learning,
a key concern is that of \emph{bias}: are
\toolname's models specific
to our data set, would they fail on
\emph{other} programs?
%
We address this concern in two ways.
%
First, we partition the data by year,
and show that models learned from one
year generalize to, \ie perform nearly
as well on, the programs from the other
year.
%
Second, we argue that in our setting
this bias is a \emph{feature} (and not
a bug): it allows \toolname to \emph{adapt}
to the kinds of errors that programmers
(specifically novices, who are in greatest
need of precise feedback) actually make,
rather than hardwiring the biases of
experts who % , by dint of their
% training and experience,
may suffer from blind spots. % with regards to such problems.
%
In this regard, we are particularly pleased
that our classifiers can be trained on a
modest amount of data, \ie a single course's
worth, and envision a future where each course
comes equipped with a model of its students' errors.



%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
