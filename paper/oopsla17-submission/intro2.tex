\section{\textbf{Introduction}}
\label{sec:introduction}

% In Robin Milner's memorable words
% %
% ``types are the leaven of computer programming;
  % they make it digestible''~\cite{tapl}.
%
Types are awesome.
%
Languages like \ocaml and \haskell make
the value-proposition for types even more
appealing by using constraints to automatically
synthesize the types for all program terms
without troubling the programmer for any
annotations.
%
Unfortunately, this automation has come at a price.
Type annotations signify
the programmer's intent and help to correctly
blame the erroneous sub-term when the code is
ill-typed.
%
In the absence of such signifiers, automatic
type inference algorithms are prone to report
type errors far from their actual source
\citep{Wand1986-nw}.
%
While this can seem like a minor annoyance to
veteran programmers, \citet{Joosten1993-yx} have found
that novices often focus their attention on the \emph{location}
reported and disregard the \emph{message}.

\mypara{Localizing Errors}
%
Several recent papers have proposed ways
to improve feedback via error \emph{localization}.
%
At a high-level these techniques analyze
the set of typing constraints to find
the minimum (weighted) subset that,
if removed would make the constraints
satisfiable and hence, memory-safe~\citep{Jose:2011}
or well-typed~\citep{Zhang2014-lv,Loncaric2016-uk,Chen2014-gd,Pavlinovic2014-mr}.
The finger of blame is then pointed at the
sub-terms that yielded those constraints.
%
This minimum-weight approach suffers
from two drawbacks.
%
First, they are not \emph{extensible}:
% the weights and analyses are limited
% to attributes of the constraints, and
% not other \emph{signals} from the
% program's syntax that may be more
% germane to error localization. Furthermore,
the constraint languages and algorithms for computing
the minimum weighted subset must be
designed afresh for different kinds
of type systems and constraints \citep{Loncaric2016-uk}.
%
Second, and perhaps most importantly,
they are not \emph{adaptable}: the
weights are fixed in an ad-hoc fashion, based on the
\emph{analysis designer's} notion
of what kinds of errors are more
or less likely, rather than
adapting to the kinds of mistakes
programmers actually make in practice.

\mypara{A Data-Driven Approach}
%
In this paper, we introduce \toolname
\footnote{``Numeric Analysis of Type Errors''; any resemblance to persons living or dead is purely coincidental.}
a \emph{data-driven} approach to error
localization based on supervised
learning (see \citealt{Kotsiantis2007-pj} for a survey).
%
\toolname analyzes a large corpus
of training data --- pairs of ill-typed
programs and their subsequent fixes ---
to automatically \emph{learn a model}
of where errors are most likely to
be found.
%
Given a new ill-typed program,
\toolname simply executes the model
to generate a list of potential
blame assignments ranked by likelihood.
%
We evaluate \toolname by comparing its
precision against the state-of-the-art
on a set of over 4,500 ill-typed \ocaml
programs drawn from introductory
programming classes.
%
We show that, when restricted to a
\emph{single} prediction, \toolname's data-driven
model is able to correctly predict
the exact sub-expression that should
be changed 74\% of the time,
29 points higher than \ocaml and
18 points higher than the state-of-the-art
\sherrloc tool.
%
Furthermore, \toolname's accuracy surpasses
85\% when we consider the top \emph{two}
locations and reaches 91\% if consider
the top \emph{three}.
%
We achieve these advances by identifying
and then solving three key challenges.

\mypara{Challenge 1: Acquiring Labeled Programs}
%
The first challenge for supervised learning
is to acquire a corpus of training data, in our setting
a set of ill-typed programs \emph{labeled}
with the exact sub-terms that are the actual
cause of the type error.
%
Prior work has often enlisted expert users
to manually judge ill-typed programs and
determine the ``correct'' fix
\citep[\eg][]{Lerner2007-dt,Loncaric2016-uk},
but this approach does not scale well to
a dataset large enough to support machine
learning.
%
Worse, while expert users have intimate
knowledge of the type system, they may
have a blind spot with regards to the
kinds of mistakes novices make, and
cannot know in general what novice users
intended.

Our \emph{first contribution} (\autoref{sec:overview})
is a set of more than 4,500 labeled programs,
giving us an accurate ground truth of
the kinds of errors and the (locations
of the) fixes that novices make in
practice.
%
We obtain this set by observing that
software development is an iterative
process; programmers eventually
fix their own ill-typed programs,
perhaps after multiple incorrect
exploratory attempts.
%
To exploit this observation we instrumented
the \ocaml compiler to collect fine-grained
traces of student interactions over two instances
of an undergraduate Programming Languages
course.\footnote{
% \begin{anonsuppress}
% UC San Diego (IRB \#140608).
% \end{anonsuppress}
% \begin{noanonsuppress}
at AUTHOR's INSTITUTION (IRB HIDDEN).
% \end{noanonsuppress}
}
%
We then post-process the resulting time-series
of programs submitted to the \ocaml compiler into
a set of pairs of ill-typed programs and their
subsequent \emph{fixes}, the first (type-)~correct
program in the trace suffix.
%
Finally, we compute the blame labels using a
\emph{tree-diff} between the two terms to find
the exact sub-terms that were changed in the fix.

\mypara{Challenge 2: Modeling Programs as Vectors}
%
Modern supervised learning algorithms work on %sets of
\emph{feature vectors}: real-valued points in an
$n$-dimensional space. While there are standard
techniques for computing such
vectors for documents, images, and sound (respectively
word-counts, pixel-values, and frequencies),
% it has
% hitherto been unclear how to precisely model program
% structure as numeric data.
there are no similarly standard representations for
programs.

Our \emph{second contribution} (\autoref{sec:learning})
solves this problem with a simple, yet expressive, representation called
a \emph{Bag-of-Abstracted-Terms (BOAT)} wherein
each program is represented by the \emph{bag}
or multiset of (sub-)~terms that appears inside
it; and further, each (sub-)~term is \emph{abstracted}
as a feature vector comprising the numeric values
returned by \emph{feature abstraction} functions
that applied to the term.
%
We can model the blame labels with a simple feature
abstraction (\eg is-changed-in-fix).
%
We can even account for \emph{contextual} information
like the features of the parent- and child- terms by
\emph{concatenating} the feature vectors of each term
with those of its parent and children.
%
We have found this representation to be particularly
convenient as it gives us flexibility in modeling the
syntactic and semantic structure of programs while
retaining compatibility with off-the-shelf classifiers,
in contrast to, \eg, \citet{Raychev2015-jg}, who had
to develop their own variants of classifiers to obtain
their results.

\mypara{Challenge 3: Training Precise Classifiers}
%
Finally, the last and most important challenge is to
use the labeled BOAT data to train classifiers that
are capable of \emph{precisely} pinpointing the errors
in real programs.
%
The key here is to find the right set of feature
abstractions and classification algorithms that
lead to precise predictions.
%
Fortunately, our BOAT model allows us a great deal of
latitude in our choice of features.
%
We can use abstraction functions to capture different
aspects of a term ranging from
%
syntactic features (\eg is-a-data constructor, is-a-literal,
is-an-arithmetic-operation, is-a-function-application, \etc),
%
to semantic features captured by the type system (\eg is-a-list,
is-an-integer, is-a-function, \etc).

Our \emph{third contribution} (\autoref{sec:evaluation})
is a systematic evaluation of our data-driven approach
using different classes of features like the above, and
with three different classification algorithms: logistic
regression, decision trees, and neural networks.
%
Our evaluation lets us identify the best features and
classifier and empirically characterize the importance
of different features for error localization.
%
In particular, we find that while machine learning
over syntactic features of each term in isolation
performs worse than existing
purely constraint based approaches (\eg \ocaml, \sherrloc),
augmenting the data with a single feature corresponding to
the \emph{type error slice} \citep{Tip2001-qp} brings our
classifiers up to par with the state of the art,
and further augmenting the data with \emph{contextual}
features allows our classifiers to outperform
the state of the art by 18 percentage points.
% In particular, we find that while machine learning
% on its own performs slightly worse than existing
% purely constraint based approaches (\eg \ocaml, \sherrloc),
% once we augment the data with feature abstraction
% corresponding to the \emph{type error slice} \citep{Tip2001-qp},
% the resulting classifier significantly outperforms the
% state of the art by 18 percentage points.

Thus, by combining modern statistical methods
with domain-specific feature engineering, \toolname
opens the door to a new data-driven path to
precise error localization.
%
In the future, we could \emph{extend}
\toolname to new languages or forms
of correctness checks by swapping in
a different set of feature abstraction
functions.
%
Furthermore, our data-driven approach
allows \toolname to \emph{adapt} to
the kinds of errors that programmers
(specifically, novices who are in greatest
need of precise feedback) actually make
rather than hardwiring the biases of
compiler authors who, by dint of their
training and experience, may suffer from
blind spots with regards to such problems.
%
In contrast, our results show that \toolname's
data-driven diagnosis can be an effective
technique for localizing errors by collectively
learning from past mistakes.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
