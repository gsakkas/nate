\section{\textbf{Introduction}}
\label{sec:introduction}

% In Robin Milner's memorable words
% %
% ``types are the leaven of computer programming;
  % they make it digestible''~\cite{tapl}.
%
Types are awesome.
%
Languages like \ocaml and \haskell make
the value-proposition for types even more
appealing by using constraints to automatically
synthesize the types for all program terms
without troubling the programmer for any
annotations.
%
Unfortunately, this automation has come at a price.
Verbose type annotations (as in, \eg, Java) signify
the programmer's intent and help to correctly
blame the erroneous sub-term when the code is
ill-typed.
%
In contrast, lacking such signifiers, automatic
type inference algorithms are prone to report
type errors quite far from their actual source.
%
While this can seem like a minor annoyance to
veteran programmers, it turns out to be a significant
stumbling block for novices~\citep{Wand1986-nw,Joosten1993-yx}.

\mypara{Localizing Errors}
%
Several recent papers have proposed ways
to improve feedback via error \emph{localization}.
%
At a high-level these techniques analyze
the set of typing constraints to find
the minimum (weighted) subset that,
if removed would make the constraints
satisfiable and hence, memory-safe~\citep{Jose:2011}
or well-typed~\citep{Zhang2014-lv,Loncaric2016-uk,Chen2014-vm,Pavlinovic2014-mr}.
The finger of blame is then pointed at the
sub-terms that yielded those constraints.
%
This minimum-weight approach suffers
from two drawbacks.
%
First, they are not \emph{extensible}:
the weights and analyses are limited
to attributes of the constraints, and
not other \emph{signals} from the
program's syntax that may be more
germane to error localization. Furthermore,
the algorithms for computing
the minimum weighted subset must be
designed afresh for different kinds
of type systems and constraints.
%
Second, and perhaps most importantly,
they are not \emph{adaptable}: the
weights are fixed in an ad-hoc fashion, based on the
\emph{analysis designers} notion
of what kinds of errors are more
or less likely, rather than
adapting to the kinds of mistakes
programmers actually make in practice.

\mypara{A Data-Driven Approach}
%
In this paper, we introduce \toolname
\footnote{``Numeric Analysis of Type Errors''; any resemblance to persons living or dead is purely coincidental.}
a \emph{data-driven} approach to error
localization based on supervised
classification~\citep{Kotsiantis2007-pj}.
%
\toolname works by using a large corpus
of training data --- pairs of ill-typed
programs and their fixed versions ---
to automatically \emph{learn a model}
of where the error is most likely.
%
When given a new ill-typed program,
\toolname simply executes the model
to generate a list of likely error
locations ranked by likelihood.
%
We evaluate \toolname by comparing its
precision against the state-of-the-art
on a set of over 4,500 ill-typed Ocaml
programs drawn from introductory
programming classes.
%
We show that \toolname's data-driven
model is able to correctly predict
the exact sub-expression that needs to
be changed, 74\% of the time, which is
29 points higher than \ocaml, and
18 points higher than the state-of-the-art
\sherrloc tool.
%
Further, \toolname's accuracy surpasses
85\% when we consider the \emph{top-2}
locations and reaches 91\% if consider
the \emph{top-3}.
%
We achieve these advances by identifying
and then solving three key challenges.

\mypara{Challenge 1: Acquiring Labeled Programs}
%
The first challenge for supervised learning
is to acquire a corpus of training data, here,
a set of ill-typed programs \emph{labeled}
with the exact sub-terms that are the actual
cause of the type error.
%
Prior work has often enlisted expert users
to manually judge ill-typed programs and
determine the ``correct'' fix \citep[\eg][]{Loncaric2016-uk},
but this approach does not scale well to
a dataset large enough to support machine
learning.
%
Worse, while expert users have intimate
knowledge of the type system, they may
have a blind spot with regards to the
kinds of mistakes novices make, and
cannot know in general what novice users
intended.

Our \emph{first contribution} (\autoref{sec:overview})
is a set of more than 4,500 labeled programs,
gives us an accurate ground truth of
the kinds of errors and the (locations
of the) fixes that novices make in
practice.
%
We obtain this set by observing that
software development is an iterative
process and programmers eventually
fix their own ill-typed programs,
perhaps after multiple incorrect
exploratory attempts.
%
To exploit this observation we instrumented
the \ocaml compiler to collect fine-grained
traces of student interactions over two instances
of an undergraduate Programming Languages course
%
\footnote{
% \begin{anonsuppress}
% UC San Diego (IRB \#140608).
% \end{anonsuppress}
% \begin{noanonsuppress}
at AUTHOR's INSTITUTION (IRB HIDDEN).
% \end{noanonsuppress}
}
%
We can then post-process the resulting time-series
of programs submitted to the \ocaml compiler into
a set of pairs of ill-typed programs and their
\emph{fixed} versions, the \emph{first} (type-)correct
program in the trace suffix.
%
We obtain the location labels by computing a
\emph{tree-diff} between the two terms to find
the exact sub-terms that were changed in the fix.

\mypara{Challenge 2: Modeling Programs as Vectors}
%
Modern supervised learning algorithms work on sets of
\emph{feature vectors}: real-valued points in an
$n$-dimensional space. While it is easy to compute such
vectors for documents, images and sound (respectively
word-counts, pixel-values and frequencies), it has
hitherto been unclear how to precisely model program
structure as numeric data.

Our \emph{second contribution} (\autoref{sec:learning})
solves this problem with a new representation called
a \emph{Bag-of-Abstract-Terms (BOAT)} wherein
%
(1)~each program is represented by the \emph{bag}
    or multiset of (sub-)terms that appears inside
    it, and
%
(2)~each (sub-)term $t$ is \emph{abstracted} as
    a vector $[f_1(t), \ldots, f_n(t)]$ where
    each $f_i$ is a \emph{feature abstraction}
    that maps a term to a numeric value.
%
We can model the error labels with a simple feature
abstraction (\eg is-a-diff-subterm).
%
We can even account for \emph{contextual} information like
the features of the parent- and children- terms by
concatenating the feature vectors of each term with those
of its parent and children.

\mypara{Challenge 3: Training Precise Classifiers}
%
Finally, the last and most important challenge is to
use the labeled BOAT data to train classifiers that
are capable of \emph{precisely} pinpointing the error
locations in real programs.
%
The key here is to find the right set of feature
abstractions and classification algorithms, that
lead to precise predictions.
%
Fortunately, our BOAT model allows us a great deal of
latitude in our choice of features.
%
We can use abstraction functions to capture different
aspects of a term ranging from
%
syntactic features (\eg is-a-data constructor, is-a-literal,
is-an-arithmetic-operation, is-a-function-application \etc),
%
to semantic features captured by the type system (\eg is-a-list,
is-an-integer, is-a-function \etc).

Our \emph{third contribution} (\autoref{sec:evaluation})
is a systematic evaluation of our data-driven approach
using different classes of features like the above, and
with three different classification algorithms: linear
classifiers, decision forests and deep neural networks.
%
Our evaluation lets us identify the best features and
classifier and empirically characterize the importance
of different features for error localization.
%
In particular, we find that while machine learning
on its own performs slightly worse than existing
purely constraint based approaches (\eg \ocaml, \sherrloc),
once we augment the data with feature abstraction
corresponding to the \emph{type error slice} \cite{Tip2001-qp},
the resulting classifier significantly outperforms
state-of-the-art by 18 percentage points.

Thus, by combining modern statistical methods
with domain-specific feature engineering, \toolname
opens the door to a new data-driven path to
precise error localization.
%
In the future, we could \emph{extend}
\toolname to new languages or forms
of correctness checks by swapping in
a different set of feature abstraction
functions.
%
Furthermore, our data-driven approach
allows \toolname to \emph{adapt} to
the kinds of errors that programmers
(specifically, novices who are in greatest
need of precise feedback) actually make
rather than hardwiring the biases of
compiler authors who, by dint of their
training and experience, may suffer from
blind spots with regards to such problems.
%
In contrast, our results show that \toolname's
data-driven diagnosis can be an effective
technique for localizing errors by collectively
learning from past mistakes.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
