\section{Overview}
\label{sec:overview}

We start with an overview of our approach to localizing type errors by
learning a model of the mistakes programmers actually make.
%
We formulate the problem of type error localization as a
\emph{supervised classification} problem (see \citealt{Kotsiantis2007-pj}
for a survey).
%
A \emph{classification} problem entails learning a function that maps
inputs to a discrete set of output \emph{labels}, in contrast to
\emph{regression}, where the output is typically a real number.
%
In a \emph{supervised} learning problem one is given a \emph{training}
set where the inputs and labels are known, and the task is to learn a
function that accurately maps the inputs to outputs and
\emph{generalizes} to new, yet-unseen inputs.

The key technical challenges we address are:
%
(1) formulating type error localization as a classification problem;
%
(2) generating correct labels for supervised learning; and
%
(3) presenting the predictions of the classifier in an intuitive manner.

\paragraph{\textbf{Challenge (1): How to represent programs?}}
The first and prime challenge we address is how to translate programs
into a format favorable to machine learning. 
%
Programs can be represented in many ways, from the raw text to 
richly-structured abstract syntax trees and control-flow graphs, but 
representations are not directly amenable to machine learning.
%
Machine learning techniques typically take as input \emph{feature
vectors}, fixed-length vectors of real numbers where each element
represents some property of the input.
%
Thus, to apply machine learning to our problem we must
determine:
%
(1) what are interesting features of a program; and
%
(2) how can we represent a variable-size program as a fixed-length vector?

\paragraph{\textbf{Solution: Vectorized Abstract Syntax Trees}}
\ES{name up for grabs...}
We propose to extract features from the program's
abstract syntax tree (AST), a natural choice since the type checker
operates on ASTs.
%
Our features are \emph{queries} over the AST. Our initial set of
interesting features includes the syntactic class of each expression.
%
That is, the classifier should be able to distinguish between the |[]|
and |+| expressions in \autoref{fig:sumList} because they represent
different syntactic expression forms.
%
A programming language has a fixed set of AST elements,\footnote{We will
  discuss handling arbitrary, user-defined types in
  \autoref{sec:discussion}.} which suggests a natural way to obtain
fixed-size feature vectors. Rather than translating a program into a
single feature vector, we translate it into a \emph{set} of feature
vectors, one for each expression in the program.
%
Thus, the |[]| and |+| expressions will each be given their own feature
vector, and we will ask the classifier to predict for each
\emph{independently} whether it should be blamed.

Of course, each expression occurs in some \emph{context}, and we would
like the classifier to be able make decisions based on the context as
well.
%
For example, we might expect that the recursive call |sumList t| is
unlikely to be at fault because |sumList| has a function type --- the
user may have called the wrong function or supplied the wrong argument,
but the application itself is probably correct.
%
Conversely, if the student wrote |h sumList t| (\ie forgot the |+|), we
might wish to blame the application rather than |h| because |h|
\emph{does not} have a function type.
%
Thus we extend each expression's feature vector with
\emph{contextual features} of its parent and children.
%
These contextual features include the syntactic class of the neighboring
expressions, their inferred types (when available), and, crucially, whether
the expression occurs in a minimal type error \emph{slice}.
%
A minimal type error slice \citep{Haack2003-vc} includes all expressions that
are necessary for the error to manifest and no more.
%
We propose to use type error slices to communicate to the classifier
which expressions could potentially be blamed --- a change to an
expression outside of the minimal slice cannot possibly fix the type
error.
%
We will show that the type error slice is so important
(\autoref{sec:feature-utility}) that it is actually beneficial to
automatically discard expressions that are not part of the slice, rather
than letting the classifier learn to do so.
%
Our use of type error slices is conceptually related to the use of fault
localization in other program analysis and transformation techniques
(see \autoref{sec:related-work}).

\paragraph{\textbf{Challenge (2): What is the ``correct'' expression to blame?}}
As we saw in \autoref{fig:sumList}, type errors admit many different
fixes to different expressions, each resulting in a different program.
%
How is the type checker to judge which expressions are more likely to be
incorrect than others?
%
Prior work has often enlisted expert users to manually judge ill-typed
programs and determine the ``correct'' fix \citep[e.g.][]{Loncaric2016-uk}, but
this approach does not scale well to a dataset large enough to support
machine learning.
%
Furthermore, while expert users have intimate knowledge of the type system,
they cannot know in general what novice users \emph{intended} to do.

\paragraph{\textbf{Solution: Use the students' subsequent fixes}}
%
Rather than relying on expert users, we allow our students to do the
work of assigning blame themselves.
%
Software development is an iterative process and our students 
eventually fix their own programs, perhaps after multiple ill-fated
attempts.
%
For each ill-typed program in our dataset, we find the first
\emph{subsequent} program that type checks and declare that to be the
fix.
%
From this pair of an ill-typed program and its fix, we can extract a
\emph{diff} of the abstract syntax trees.
%
For example, suppose our student fixed the |sumList| program in
\autoref{fig:sumList} by replacing |[]| with |0|, the diff would include
only the |[]| expression.
%
Thus we would determine that the |[]| expression (and \emph{not} the
|+| or the recursive call |sumList t|) is to blame for the error.


\paragraph{\textbf{Challenge (3): What should be presented to the user?}}
We now have the features and labels needed to train a classifier to
predict, for each expression in an ill-typed program, whether it should
be blamed.
%
But this is not yet particularly suitable as \emph{user feedback}.
%
A recent survey of developers by \citet{Kochhar2016-oc} found that
developers are unlikely to examine more than around five potentially
erroneous locations before falling back to manual debugging.
%
Thus, we should limit our predictions to a select few to be presented to
the user.

\paragraph{\textbf{Solution: Rank blame locations by ``confidence''}}
Luckily, many machine learning classifiers produce not only a predicted
label, but also a metric that can be interpreted as the classifier's
\emph{confidence} in its prediction.
%
Thus, we \emph{rank} each expression by the classifier's confidence that
it should be blamed, and present only the top-$k$ predictions to the
user (in practice $k=3$). This use of ranking to report
the results of a program analysis is popular in other problem domains
\citep[see, \eg][]{Kremenek2003-ck}; we focus
explicitly on the use of data-driven machine learning confidence as a
ranking source.



%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
