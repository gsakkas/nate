\mysubsection{Threats to Validity}
\label{sec:validity}

Although our experiments demonstrate that our technique can pinpoint type
errors more accurately than the state of the art and that our features are
relevant to blame assignment, our results may not generalize.

One threat to validity associated with supervised machine learning is
overfitting (\ie learning a model that is too complex with respect to
the data).
%
A similar issue that arises in machine learning is model stability (\ie
can small changes to the training set produce large changes in the model?).
%
We mitigate these threats by:
%
(1) using separate training and testing datasets drawn from distinct
student populations (\autoref{sec:quantitative}), demonstrating the
generality of our models; and
%
(2) via cross-validation on the joint dataset
(\autoref{sec:feature-utility}), which demonstrates the stability of our
models by averaging the accuracy of 10 models trained on distinct
subsets of the data.

Our benchmarks were drawn from students in an undergraduate course and
may not be representative of other student populations.
%
We mitigate this threat by including the largest empirical evaluation of
type error localization that we are aware of: over 4,500 pairs of
ill-typed programs and fixes from two instances of the course, with
programs from 102 different students.
%
We acknowledge, of course, that students are not industrial programmers
and our results may not translate to large-scale software development;
however, we are particularly interested in aiding novice programmers
as they learn to work inside the type system.

A related threat to construct validity is our definition of the immedate
next well-typed program as the intended ground truth answer (see
\autoref{sec:overview}, Challenge 2). Students may, in theory, submit
intermediate well-typed program ``rewrites'' between the original ill-typed
program and the final intended answer. Our approach to discarding outliers
(see \autoref{sec:evaluation}) is designed to mitigate this threat.

Our removal of program pairs that changed too much, where our oracle
could not identify the blame of the other tools, or where the other
tools timed out or encountered unsupported language features is another
threat to validity.
%
It is possible that including the programs that changed excessively
would hurt our models, or that the other tools would perform
better on the programs with unsupported language features.
%
We note however that
%
(1) outlier removal is a standard technique in machine learning%
%\ES{CITE?}
; and
%
(2) our Top-1 accuracy margin is large enough that even if we assumed
that \sherrloc were perfect on all excluded programs,
% it would only tie our \hiddenFH. % in Top-1 accuracy.
we would still lead by 10 points.
%
%\ES{I think this is accurate, but should double check..}

Examining programs written in \ocaml as opposed to \haskell or any other
typed functional language poses yet another threat, common type errors
may differ in different languages.
%
\ocaml is, however, a standard target for research in type error
localization and thus our choice admits a direct comparison with prior
work.
%
Furthermore, the functional core of \ocaml that we support does not
differ significantly from the functional core of \haskell or SML, all of
which are effectively lambda calculi with a Hindley-Milner-style type
system.
% \footnote{\haskell's type classes are a notable exception, they
%   are also known to cause confusing type errors and would be interesting
%   to study as well.}

Finally, our use of student fixes as oracles
% for the source of type errors
assumes that students are able to correctly identify
the source of an error.
%
As the students are in the process of learning the language and type
system, this assumption may be faulty.
%
It may be that \emph{expert} users would disagree with many of the
student fixes, and that it is harder to learn a model of expert fixes,
or that the state of the art would be better at predicting expert fixes.
%
As we have noted before, we believe it is reasonable to use student
fixes as oracles because the student is the best judge of what she
\emph{intended}.
