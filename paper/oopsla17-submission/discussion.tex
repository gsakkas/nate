
\mysection{Limitations and Threats to Validity}
\label{sec:discussion}

We have shown that we can outperform the state of the art in type error
localization by learning a model of the errors that programmers make,
using a set of features that closely resemble the information the type
checker sees.
%
However, there is certainly room for improvement.
%
In this section we highlight some limitations of our approach and
potential avenues for future work.

\mysubsection{Limitations}
\label{sec:limitations}

\mypara{User-Defined Types}
Probably the single biggest limitation of our technique is that we have
(a finite set of) features for specific data and type constructors.
%
Anything our models learn about errors made with the |::| constructor or
the |list| type cannot easily be translated to new, user-defined
datatypes the model has never encountered.
%
We can mitigate this, to some extent, by adding generic
syntactic features for data constructors and match expressions, but it
remains to be seen how much these help \ES{TODO: UW data!}.
%
Furthermore, there is no obvious analog for transferring knowledge to
new type constructors, which we have seen are both more compact and
helpful.

As an alternative to encoding information about \emph{specific}
constructors, we might investigate a more abstract representation.
%
For example, instead of modeling |x :: 2| as a |::| constructor with a
right child of type |int|, we might model it as some (unknown) constructor
whose right child has an incompatible type.
%
We might symmetrically model the |2| as an integer literal whose type is
incompatible with its parent.
%
Anything we learn about |::| and |2| can now be transferred directly to
yet unseen types, but we run the risk generalizing \emph{too much} ---
\ie perhaps programmers make different mistakes with |list|s than they
do with other types, and are thus likely to choose different fixes.
%
Balancing the trade-off between specificity and generalizability appears
to be a challenging task.

% \begin{itemize}
% \item Hard to support in general, as each data and type constructor gets
%   its own set of syntactic features.
% \item Attempt to mitigate by adding generic ``is-datacon'' and
%   ``is-match'' feature, unclear how much it will help
% \end{itemize}

\mypara{Additional Features}
There are a number of other features that could improve the model's
ability to localize errors, that would be easier to add than
user-defined types.
%
For example, each occurrence of a variable knows only its type and the
immediate context in which it occurs, but it may be helpful to know
about \emph{other} occurrences of the same variable.
%
If a variable is generally used in the context of a |float| but has a
single occurrence in the context of an |int|, it seems likely that the
latter occurrence (or context) is to blame.
%
Similarly, arguments to a function application are not aware of the
typing constraints imposed on them by the function (and vice versa),
they only know that they are occurring in the context of an application
and the type of the application itself.
%
Finally, \emph{n-grams} on the token stream have proven effective for
probabilistic modeling of programming languages
\citep{Hindle2012-hf,Gabel2010-el}, we may find that they aid in
our task as well.
%
For example, if the observed tokens in an expression diverge from the
n-gram model's predictions, that indicates that there is something
unusual about the program at that point, and it may signal an error.

% \begin{itemize}
% \item def-use chains
% \item connect function and arguments in application (right now each only know about the application itself)
% \item ??
% \end{itemize}

\mypara{Independent vs Joint Predictions}
We treat each sub-expression as if it exists in a vacuum, but in reality
the program has a rich \emph{graphical} structure, particularly if one adds
edges connecting different occurrences of the same variable.
%
\citet{Raychev2015-jg} have used these richer models to great effect to
make \emph{interdependent} predictions about programs, \eg
de-obfuscating variable names or even inferring types.
%
One could even view our task of locating the source of an error as simply
another property to be predicted over a graphical model of the program.
%
One of the key advantages of a graphical model is that the predictions
made for one node can influence the predictions made for another node,
this is known as \emph{structured learning}.
%
For example, if, given the expression |1 + true|, we predict |true| to
be erroneous, we may be much less likely to predict |+| as erroneous.
%
We compensate somewhat for our lack of structure by adding contextual
features and by ranking our predictions by ``confidence'', but it would
be interesting to see how structured learning over graphical models
would perform.


\mysubsection{Threats to Validity}
\label{sec:validity}

Although our experiments demonstrate that our technique can pinpoint type
errors more accurately than the state of the art and that our features are
relevant to blame assignment, our results may not generalize.

One threat to validity associated with supervised machine learning is
overfitting (\ie learning a model that is too complex with respect to
the data).
%
A similar issue that arises in machine learning is model stability (\ie
can small changes to the training set produce large changes in the model?).
%
We mitigate these threats by:
%
(1) using separate training and testing datasets drawn from distinct
student populations (\autoref{sec:quantitative}), demonstrating the
generality of our models; and
%
(2) via cross-validation on the joint dataset
(\autoref{sec:feature-utility}), which demonstrates the stability of our
models by averaging the accuracy of 10 models trained on distinct
subsets of the data.

Our benchmarks were drawn from students in an undergraduate course and
may not be representative of other student populations.
%
We mitigate this threat by including the largest empirical evaluation of
type error localization that we are aware of: over 4,500 pairs of
ill-typed programs and fixes from two instances of the course, with
programs from 102 different students.
%
We acknowledge, of course, that students are not industrial programmers
and our results may not translate to large-scale software development;
however, we are particularly interested in aiding novice programmers
as they learn to work inside the type system.

A related threat to construct validity is our definition of the immedate
next well-typed program as the intended ground truth answer (see
\autoref{sec:overview}, Challenge 2). Students may, in theory, submit
intermediate well-typed program ``rewrites'' between the original ill-typed
program and the final intended answer. Our approach to discarding outliers
(see \autoref{sec:evaluation}) is designed to mitigate this threat.

Our removal of program pairs that changed too much, where our oracle
could not identify the blame of the other tools, or where the other
tools timed out or encountered unsupported language features is another
threat to validity.
%
It is possible that including the programs that changed excessively
would hurt our models, or that the other tools would perform
better on the programs with unsupported language features.
%
We note however that
%
(1) outlier removal is a standard technique in machine learning
\ES{CITE?}; and
%
(2) our Top-1 accuracy margin is large enough that even if we assumed
that \sherrloc were perfect on all of the excluded programs, it would
only tie our \hiddenFH in Top-1 accuracy.
%
\ES{I think this is accurate, but should double check..}

Examining programs written in \ocaml as opposed to \haskell or any other
typed functional language poses yet another threat, common type errors
may differ in different languages.
%
\ocaml is, however, a standard target for research in type error
localization and thus our choice admits a direct comparison with prior
work.
%
Furthermore, the functional core of \ocaml that we support does not
differ significantly from the functional core of \haskell or SML, all of
which are effectively lambda calculi with a Hindley-Milner-style type
system.\footnote{\haskell's type classes are a notable exception, they
  are also known to cause confusing type errors and would be interesting
  to study in future work.}

Finally, our use of student fixes as oracles for the source of type
errors assumes that, on average, students are able to correctly identify
the source of an error.
%
As the students are in the process of learning the language and type
system, this assumption may be faulty.
%
It may be that \emph{expert} users would disagree with many of the
student fixes, and that it is harder to learn a model of expert fixes,
or that the state of the art would be better at predicting expert fixes.
%
As we have noted before, we believe it is reasonable to use student
fixes as oracles because the student is the best judge of what she
\emph{meant} to do.


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
