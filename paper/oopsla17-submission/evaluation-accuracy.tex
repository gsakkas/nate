\subsection{Blame Accuracy}
\label{sec:quantitative}

In this experiment we compare the accuracy of our predictions to the
state of the art in type error localization.

\paragraph{Baseline}
We provide two baselines for the comparison: a random choice of location
from the minimized type error slice, and the standard \ocaml compiler.

\paragraph{State of the Art}
\mycroft~\citep{Loncaric2016-uk} localizes type errors by searching for
a minimal subset of typing constraints that can be removed, such that
the resulting system is satisfiable.
%
When multiple such subsets exist it can enumerate them, though it has no
notion of which subsets are \emph{more likely} to be correct, and thus
the order is arbitrary.
%
\sherrloc~\citep{Zhang2014-lv} localizes errors by searching the typing
constraint graph for constraints that participate in many unsatisfiable
paths and few satisfiable paths.
%
It can also enumerate multiple predictions, in descending order of
likelihood.

Comparing source locations from multiple tools with their own parsers is
not trivial.
%
To ensure a fair comparison when evaluating \mycroft and
\sherrloc, we removed from the dataset programs where they predicted
locations that our oracle could not match with a program expression:
6--8\% of programs for \mycroft and 3--4\% for \sherrloc.
%
We also do not consider programs where \mycroft or \sherrloc timed out
(after one minute) or where they encountered an unsupported language
feature: another 5\% for \mycroft and 12--13\% for \sherrloc. This
experimental design gives the state of the art tools the ``benefit of the
doubt''.


\paragraph{Our Classifiers}
We evaluate five classifiers, each trained on the full set of features.
% features: 44 local syntactic features, 176 contextual syntactic
% features, 55 typing features, and a single expression size feature.
% %
% \ES{should explain the make-up of these groups}
%
% We preemptively discard expressions that are not part of the minimal
% type error slice --- we will explain the rationale for this in
% \autoref{sec:feature-utility} --- and thus the final feature count is
% 276.
%
Our classifiers are:
%
\begin{description}
\item[\linear] A logistic regression trained with a learning rate
  $\eta = 0.001$, an $L_2$ regularization rate $\lambda = 0.001$, and a
  mini-batch size of 200.
\item[\dectree] A decision tree trained with the CART algorithm
  \citep{Breiman1984-qy} and an impurity threshold of $10^{-7}$ (used to
  avoid overfitting via early stopping).
\item[\forest] A random forest \citep{Breiman2001-wo} of 30
  estimators, trained with an impurity threshold of $10^{-7}$.
\item[\hiddenT and \hiddenFH] Two multi-layer perceptron neural
  networks, both trained with $\eta = 0.001$, $\lambda = 0.001$, and a
  mini-batch size of 200. The first MLP contains a single hidden layer
  of 10 neurons, and the second contains a hidden layer of 500
  neurons. This allows us to investigate how well the MLP can
  \emph{compress} its model (cf.~\cite{FIXME}). The neurons use
  rectified linear units (ReLU) as their activation function, a common
  practice in modern neural networks.
\end{description}
%
All classifiers were trained for 20 epochs on one dataset
--- \ie they were shown each program 20 times ---
before being evaluated on the other.
%
The logistic regression and MLPs were trained with the \textsc{Adam}
optimizer \citep{Kingma2014-ng}, a variant of stochastic gradient
descent that has been found to converge faster.

\input{evaluation-accuracy-graph}

\paragraph{Results}
\autoref{fig:accuracy-results} shows the results of our experiment.
%
Localizing the type errors in our benchmarks amounted, on average, to
selecting one of 3 correct locations out of a slice of 10.
%
Our baseline of selecting at random achieves 30\% Top-1
accuracy (58\% Top-3), while \ocaml achieves a Top-1 accuracy of 45\%.
%
Interestingly, one only needs two \emph{random} guesses to outperform
\ocaml, with 47\% accuracy.
%
\sherrloc outperforms both baselines with 56\% Top-1 accuracy (84--86\% Top
3), while \mycroft actually underperforms \ocaml with 38--41\% Top-1
accuracy.
%
Finally, we find that \emph{all} of our classifiers outperform \sherrloc,
ranging from 58--62\% Top-1 accuracy (86--88\% Top-3) for the \linear
classifier to 71--74\% Top-1 accuracy (91\% Top-3) for the \hiddenFH.

Surprisingly, there is little variation in accuracy between classifiers.
With the exception of the \linear model, they all achieve around 70\%
Top-1 accuracy and around 90\% Top-3 accuracy.
%
This suggests that the model they learn is relatively simple.
%
In particular, notice that although the \hiddenT has $50\times$ \emph{fewer}
hidden neurons than the \hiddenFH, it only loses around 2\% accuracy.
% In particular, notice that the \hiddenT only loses around 2\% accuracy
% compared to the \hiddenFH,
%
We also note that our classifiers consistently perform better when
trained on the \FALL programs and tested on the \SPRING programs than
vice versa.
% , they appear to generalize better from the \FALL data.
% FIXME: Why? What is your explanation for this? Is it just sizes of those
% datasets or something qualitative about the program pairs in them?
