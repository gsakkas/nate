\subsection{Feature Utility}
\label{sec:feature-utility}
We have shown that we can train a classifier to effectively localize
type errors, but which of the 276 features that we use are contributing
the most?
%
In order to answer this question we investigate the performance of
classifiers trained on various subsets of the features.

\subsubsection{Type Error Slice}
\label{sec:type-error-slice}
The \InSlice feature should be highly predictive --- a fix must change
at least one expression in the type-error slice.
%
Thus, our first experiment seeks to quantify the impact of \InSlice by
comparing the accuracy of a linear model on three sets of features:
%
\begin{enumerate}
\item A baseline with only local syntactic features.
\item The local syntactic features of (1) extended with \InSlice.
\item The same features as (1), but we preemptively discard samples
  where \InSlice is \emph{disabled}.
\end{enumerate}
%
The key difference between (2) and (3) is that a classifier for (2) must
\emph{learn} that \InSlice is a strong predictor.
%
In contrast, a classifier for (3) must only learn about the syntactic
features, the decision to discard samples where \InSlice is disabled has
already been made by a human.
%
This has a few additional advantages: it reduces the set of candidate
locations by a factor of 7 on average, and it guarantees that any
prediction made by the classifier can fix the type error.
%
We expect that (2) will perform better than (1) as it contains more
information, and that (3) will perform better than (2) as the classifier
does not have to learn the importance of \InSlice.

We tested our hypothesis with a linear model cross-validated ($k=10$)
over the combined SP14/FA15 dataset. We used a learning rate
$\alpha=0.001$, L2 regularization rate $\lambda=0.001$, and mini-batch
size of 200. We trained for a single epoch on feature sets (1) and
(2), and for 8 epochs on (3), so that the total number of training samples
would be roughly equal for each feature set.

\begin{table}[ht]
  \centering
  \begin{tabular}{lrcrrrrcrrrr}
    \toprule
                 &             & & \multicolumn{4}{c} \linear        & & \multicolumn{4}{c} \hiddenF       \\
                                   \cmidrule{4-7}                        \cmidrule{9-12}
    Feature Set  & \# Features & & Top-1  & Top-2  & Top-3  & Recall & & Top-1  & Top-2  & Top-3  & Recall \\
    \midrule
    Local Syntax & 47          & & 23.6\% & 42.6\% & 56.3\% & 19.6\% & & 30.9\% & 47.9\% & 58.6\% & 20.6\% \\
    + Slice      & 48          & & 46.4\% & 65.0\% & 75.4\% & 30.4\% & & 54.5\% & 70.5\% & 81.5\% & 34.7\% \\
    Only Slice   & 47          & & 54.9\% & 71.4\% & 82.5\% & 57.6\% & & 56.7\% & 72.3\% & 82.9\% & 58.0\% \\
    \bottomrule
  \end{tabular}
  \caption{
    Impact of type-error slice on accuracy.
    \ES{TODO: expand caption}
    \ES{TODO: load these numbers from CSV}
  }\label{tab:type-error-slice}
\end{table}

\autoref{tab:type-error-slice} shows the results of our experiment.
%
As expected the baseline performs the worst, with a mere 23.6\% Top-1
accuracy.
%
Adding \InSlice improves the results substantially with a 46.4\% Top-1
accuracy, demonstrating the importance of a minimal error slice.
%
However, filtering out expressions that are not part of the slice
\emph{further} improves the results to 54.9\% Top-1 accuracy.
%
Clearly, some decisions are too important to be left to a machine.

Note also the jump in Recall when we filter out expressions that are not
part of the error slice.
%
Reducing the search space not only improves our chances of making a
single correct prediction, it also allows us to make \emph{multiple}
correct predictions per program.

Given the decisive benefits of filtering out expressions that do not
belong to the type-error slice, we will assume going forward that
\emph{all} data sets have been filtered.

\subsubsection{Contextual Features}
\label{sec:contextual-features}

Next, we will investigate the relative impact of the other three classes
of features discussed in \autoref{sec:features}.
%
For this we consider again a baseline of only local syntactic features,
extended by each combination of
%
(1) expression size,
(2) contextual syntactic features, and
(3) typing features.
%
As before we perform a 10-fold cross-validation with a learning rate and
L2 regularization rate of 0.001 and a mini-batch size of 200, but we
train for a full 10 epochs.

\begin{table}[ht]
  \centering
  \begin{tabular}{lrcrrrrcrrrr}
    \toprule
                             &             & & \multicolumn{4}{c} \linear        & & \multicolumn{4}{c} \hiddenFH      \\
                                               \cmidrule{4-7}                        \cmidrule{9-12}
    Feature Set              & \# Features & & Top-1  & Top-2  & Top-3  & Recall & & Top-1  & Top-2  & Top-3  & Recall \\
    \midrule
    Local Syntax             &  44         & & 55.0\% & 71.6\% & 82.6\% & 57.7\% & & 56.3\% & 72.1\% & 82.8\% & 57.6\% \\
    \midrule
    + Size                   &  45         & & 55.8\% & 72.8\% & 82.5\% & 57.3\% & & 60.5\% & 75.0\% & 83.8\% & 57.9\% \\
    + Context                & 220         & & 59.9\% & 77.7\% & 86.4\% & 63.0\% & & 71.4\% & 84.1\% & 90.8\% & 69.5\% \\
    + Types                  & 102         & & 62.2\% & 77.7\% & 85.7\% & 62.2\% & & 73.2\% & 84.8\% & 90.2\% & 69.2\% \\
    \midrule
    + Context + Size         & 221         & & 60.2\% & 77.9\% & 86.0\% & 62.5\% & & 71.7\% & 83.5\% & 90.7\% & 69.3\% \\
    + Types + Size           & 103         & & 62.0\% & 78.2\% & 85.6\% & 62.3\% & & 73.5\% & 85.8\% & 91.4\% & 70.5\% \\
    + Context + Types        & 275         & & 63.2\% & 80.3\% & 87.9\% & 65.4\% & & 77.3\% & 87.5\% & 92.4\% & 72.9\% \\
    \midrule
    + Context + Types + Size & 276         & & 62.3\% & 80.0\% & 88.0\% & 65.4\% & & 77.2\% & 87.9\% & 92.7\% & 72.9\% \\
    \bottomrule
  \end{tabular}
  % \begin{minipage}{0.49\linewidth}
  % \centering
  % \hiddenF
  % \begin{tabular}{lrrrr}
  %   \toprule
  %   Feature Set                 & Top-1  & Top-2  & Top-3  & Recall \\
  %   \midrule
  %   Local Syntax                & 56.9\% & 72.2\% & 82.8\% & 57.9\% \\
  %   \midrule
  %   + Size                      & 59.7\% & 74.6\% & 83.0\% & 57.4\% \\
  %   + Context                   & 70.9\% & 83.7\% & 90.4\% & 69.2\% \\
  %   + Types                     & 72.1\% & 84.1\% & 90.3\% & 69.3\% \\
  %   \midrule
  %   + Size + Context            & 69.8\% & 83.5\% & 90.2\% & 68.6\% \\
  %   + Size + Types              & 72.3\% & 84.6\% & 90.3\% & 69.5\% \\
  %   + Context + Types           & 75.5\% & 86.4\% & 91.5\% & 71.7\% \\
  %   \midrule
  %   + All                       & 75.0\% & 86.8\% & 91.9\% & 72.0\% \\
  %   \bottomrule
  % \end{tabular}
  % \end{minipage}
  % \\
  % \begin{minipage}{0.49\linewidth}
  % \centering
  % \hiddenFH
  % \begin{tabular}{lrrrr}
  %   \toprule
  %   Feature Set                 \\
  %   \midrule
  %   Local Syntax                \\
  %   \midrule
  %   + Size                      \\
  %   + Context                   \\
  %   + Types                     \\
  %   \midrule
  %   + Size + Context            \\
  %   + Size + Types              \\
  %   + Context + Types           \\
  %   \midrule
  %   + All                       \\
  %   \bottomrule
  % \end{tabular}
  % \end{minipage}
  \caption{
    Impact of contextual features on accuracy.
    \ES{TODO: expand caption}
    \ES{TODO: load these numbers from CSV}
  }\label{tab:type-error-slice}
\end{table}
