\subsection{Feature Utility}
\label{sec:feature-utility}
We have shown that we can train a classifier to effectively localize
type errors, but which of the 276 features that we use are contributing
the most?
%
In order to answer this question we investigate the performance of
classifiers trained on various subsets of the features.

\subsubsection{Type Error Slice}
\label{sec:type-error-slice}
The \InSlice feature should be highly predictive --- a fix must change
at least one expression in the type error slice.
%
Thus, our first experiment seeks to quantify the impact of \InSlice by
comparing the accuracy of our classifiers on three sets of features:
%
\begin{enumerate}
\item A baseline with \emph{only} local syntactic features.
\item The features of (1) extended with \InSlice.
\item The same features as (1), but we preemptively discard samples
  where \InSlice is \emph{disabled}.
\end{enumerate}
%
The key difference between (2) and (3) is that a classifier for (2) must
\emph{learn} that \InSlice is a strong predictor.
%
In contrast, a classifier for (3) must only learn about the syntactic
features, the decision to discard samples where \InSlice is disabled has
already been made by a human.
%
This has a few additional advantages: it reduces the set of candidate
locations by a factor of 7 on average, and it guarantees that any
prediction made by the classifier can fix the type error.
%
We expect that (2) will perform better than (1) as it contains more
information, and that (3) will perform better than (2) as the classifier
does not have to learn the importance of \InSlice.

We tested our hypothesis with the \linear and
%
\hiddenFH\footnote{A layer of 500 neurons is excessive when we have so few
  input features --- we use \hiddenFH for continuity with the
  surrounding sections.}
%
classifiers, cross-validated ($k=10$) over the combined SP14/FA15
dataset. We used a learning rate $\alpha=0.001$, L2 regularization rate
$\lambda=0.001$, and mini-batch size of 200. We trained for a single
epoch on feature sets (1) and (2), and for 8 epochs on (3), so that the
total number of training samples would be roughly equal for each feature
set.
%
\lstDeleteShortInline{|} % sigh...
In addition to the accuracy metric from before, we report each
classifier's \emph{recall} --- \ie ``How many true changes can we
remember?'' --- defined as
$$
\frac{|\mathsf{predicted} \cap \mathsf{oracle}|}
     {|\mathsf{oracle}|}
$$
where $\mathsf{predicted}$ is limited to the top 3 predictions of the
classier, and $\mathsf{oracle}$ is limited to the expressions that
changed \emph{and} were part of the type error slice. We make the
latter distinction as (1) changes that are not part of the type error
slice are noise in the data set, and (2) it makes the comparison between
feature sets easier to interpret since $\mathsf{oracle}$ never changes.
% NOTE: keep this at the end of the para or it screws up spacing...
\lstMakeShortInline{|}
%
\begin{table}[ht]
  \caption{
    Impact of Type Error Slice on Accuracy.
    \ES{TODO: load these numbers from CSV}
  }\label{tab:type-error-slice}
  \centering
  \begin{tabular}{lrcrrrrcrrrr}
    \toprule
                       &             & & \multicolumn{4}{c} \linear        & & \multicolumn{4}{c} \hiddenFH      \\
                                         \cmidrule{4-7}                        \cmidrule{9-12}
    Feature Set        & \# Features & & Top-1  & Top-2  & Top-3  & Recall & & Top-1  & Top-2  & Top-3  & Recall \\
    \midrule
    Local Syntax       & 44          & & 23.5\% & 42.5\% & 55.9\% & 36.5\% & & 28.9\% & 47.7\% & 58.6\% & 38.4\% \\
    + \InSlice         & 45          & & 46.1\% & 64.9\% & 75.4\% & 48.8\% & & 54.4\% & 70.8\% & 81.3\% & 56.2\% \\
    Filter by \InSlice & 44          & & 54.9\% & 71.4\% & 82.5\% & 57.6\% & & 56.6\% & 71.9\% & 82.5\% & 57.3\% \\
    \bottomrule
  \end{tabular}
\end{table}

\autoref{tab:type-error-slice} shows the results of our experiment.
%
As expected the baseline performs the worst, with a mere 23.6\% \linear
Top-1 accuracy.
%
Adding \InSlice improves the results substantially with a 46.4\% \linear Top-1
accuracy, demonstrating the importance of a minimal error slice.
%
However, filtering out expressions that are not part of the slice
\emph{further} improves the results to 54.9\% \linear Top-1 accuracy.
%
Interestingly, while the \hiddenFH performs similarly poor with no error
slice features, it recovers nearly all of its accuracy after being given
the error slice features.
%
Top-1 accuracy jumps from 28.9\% to 54.4\% when we add \InSlice, and only
improves by 2\% when we filter out expressions that are not part of the
error slice.

Still, the 2\% boost comes at zero cost, and given the other benefits
of filtering out expressions that do not belong to the type error slice
--- shrinking the search space and guaranteeing our predictions are actionable ---
we choose to filter all programs by \InSlice.

\subsubsection{Contextual Features}
\label{sec:contextual-features}

Continuing on, we will investigate the relative impact of the other
three classes of features discussed in \autoref{sec:features}, assuming
we have discarded expressions not in the type error slice.
%
For this experiment we consider again a baseline of only local syntactic
features, extended by each combination of
%
(1) expression size,
(2) contextual syntactic features, and
(3) typing features.
%
As before we perform a 10-fold cross-validation with $\alpha = 0.001$,
$\lambda = 0.001$, and a mini-batch size of 200, but we
train for a full 20 epochs to make the differences more apparent.
%
\begin{table}[ht]
  \caption{
    Impact of Contextual Features on Accuracy.
    \ES{TODO: load these numbers from CSV}
  }\label{tab:contextual-features}
  \centering
  \begin{tabular}{lrcrrrrcrrrr}
    \toprule
                             &             & & \multicolumn{4}{c} \linear        & & \multicolumn{4}{c} \hiddenFH      \\
                                               \cmidrule{4-7}                        \cmidrule{9-12}
    Feature Set              & \# Features & & Top-1  & Top-2  & Top-3  & Recall & & Top-1  & Top-2  & Top-3  & Recall \\
    \midrule
    Local Syntax             &  44         & & 55.0\% & 71.6\% & 82.6\% & 57.7\% & & 56.3\% & 72.1\% & 82.8\% & 57.6\% \\
    \midrule
    + Size                   &  45         & & 55.8\% & 72.8\% & 82.5\% & 57.3\% & & 60.5\% & 75.0\% & 83.8\% & 57.9\% \\
    + Context                & 220         & & 59.9\% & 77.7\% & 86.4\% & 63.0\% & & 71.4\% & 84.1\% & 90.8\% & 69.5\% \\
    + Types                  & 102         & & 62.2\% & 77.7\% & 85.7\% & 62.2\% & & 73.2\% & 84.8\% & 90.2\% & 69.2\% \\
    \midrule
    + Context + Size         & 221         & & 60.2\% & 77.9\% & 86.0\% & 62.5\% & & 71.7\% & 83.5\% & 90.7\% & 69.3\% \\
    + Types + Size           & 103         & & 62.0\% & 78.2\% & 85.6\% & 62.3\% & & 73.5\% & 85.8\% & 91.4\% & 70.5\% \\
    + Context + Types        & 275         & & 63.2\% & 80.3\% & 87.9\% & 65.4\% & & 77.3\% & 87.5\% & 92.4\% & 72.9\% \\
    \midrule
    + Context + Types + Size & 276         & & 62.3\% & 80.0\% & 88.0\% & 65.4\% & & 77.2\% & 87.9\% & 92.7\% & 72.9\% \\
    \bottomrule
  \end{tabular}
  % \begin{minipage}{0.49\linewidth}
  % \centering
  % \hiddenF
  % \begin{tabular}{lrrrr}
  %   \toprule
  %   Feature Set                 & Top-1  & Top-2  & Top-3  & Recall \\
  %   \midrule
  %   Local Syntax                & 56.9\% & 72.2\% & 82.8\% & 57.9\% \\
  %   \midrule
  %   + Size                      & 59.7\% & 74.6\% & 83.0\% & 57.4\% \\
  %   + Context                   & 70.9\% & 83.7\% & 90.4\% & 69.2\% \\
  %   + Types                     & 72.1\% & 84.1\% & 90.3\% & 69.3\% \\
  %   \midrule
  %   + Size + Context            & 69.8\% & 83.5\% & 90.2\% & 68.6\% \\
  %   + Size + Types              & 72.3\% & 84.6\% & 90.3\% & 69.5\% \\
  %   + Context + Types           & 75.5\% & 86.4\% & 91.5\% & 71.7\% \\
  %   \midrule
  %   + All                       & 75.0\% & 86.8\% & 91.9\% & 72.0\% \\
  %   \bottomrule
  % \end{tabular}
  % \end{minipage}
\end{table}
%
\paragraph{Results}
\autoref{tab:contextual-features} summarizes the results of our experiment.
%
As we can see, the \linear classifier and the \hiddenFH start off
competitive when given only local syntactic features, but the \hiddenFH
quickly begins to outperform as more features are added.

\ExprSize appears to be the weakest feature, improving \linear Top-1
accuracy by only 1\% and \hiddenFH by only 4\%.
%
In contrast, the contextual syntactic features improve \linear Top-1
accuracy by 5\% (\resp 15\%), and the typing features improve
Top-1 accuracy by 7\% (\resp 17\%).
%
Furthermore, while \ExprSize does provide some benefit when it is the
only additional feature, it does not appear to provide any real increase
in accuracy when added alongside the contextual or typing features.

As one might expect, the typing features are more beneficial than the
contextual syntactic features.
%
They improve Top-1 accuracy by an additional 2\%, and are much more
compact --- we only have 55 typing features compared to 176 contextual
syntactic features.
%
This aligns with our intuition that types should be a good summary of
the context of an expression.
%
However, typing features do not \emph{subsume} contextual syntactic
features, we can gain an additional 1\% Top-1 accuracy (\resp 4\%) by
adding \emph{both}.
