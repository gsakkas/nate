\mysubsection{Feature Utility}
\label{sec:feature-utility}
We have shown that we can train a classifier to effectively localize
type errors, but which of the feature classes from
\autoref{sec:features} are contributing the most to our accuracy?
%
We focus specifically on feature \emph{classes} rather than individual
features as our 282 features are conceptually grouped into a much
smaller number of \emph{categorical} features.
%
For example, the syntactic class of an expression is conceptually a
feature but there are 45 possible values it could take; to encode this
feature for learning we split it into 45 distinct binary features.
%
Analyses that focus on individual features, \eg \textsc{ANOVA},
are difficult to interpret in our setting, as they will tell us the
importance of the binary features but not the higher-level categorical
features.
%
Thus, to answer our question we investigate the performance of
classifiers trained on various subsets of the feature classes.

\mysubsubsection{Type Error Slice}
\label{sec:type-error-slice}
First we must justify our decision to automatically exclude expressions
outside the minimal type error slice from consideration.
%
% The \InSlice feature should be highly predictive --- a fix must change
% at least one expression in the type error slice.
% %
% Thus, our first experiment seeks to quantify the impact of \InSlice by
% comparing the accuracy of our classifiers on three sets of features:
%
Thus, we compare three sets of features:
%
\begin{enumerate}
\item A baseline with only local syntactic features and no
  preemptive filtering by \InSlice.
\item The features of (1) extended with \InSlice.
\item The same features as (1), but we preemptively discard samples
  where \InSlice is disabled.
\end{enumerate}
%
The key difference between (2) and (3) is that a classifier for (2) must
\emph{learn} that \InSlice is a strong predictor.
%
In contrast, a classifier for (3) must only learn about the syntactic
features, the decision to discard samples where \InSlice is disabled has
already been made by a human.
%
This has a few additional advantages: it reduces the set of candidate
locations by a factor of 7 on average, and it guarantees that any
prediction made by the classifier can fix the type error.
%
We expect that (2) will perform better than (1) as it contains more
information, and that (3) will perform better than (2) as the classifier
does not have to learn the importance of \InSlice.

% FIXME: Wes feels that there should be a sentence in the next mypara
% explaining to the reader why we didn't just use an ANOVA or the ReliefF
% method or whatever to figure out feature importance. Feature overlap?

We tested our hypothesis with the \linear and
%
\hiddenFH\footnote{A layer of 500 neurons is excessive when we have so few
  input features --- we use \hiddenFH for continuity with the
  surrounding sections.}
%
classifiers, cross-validated ($k=10$) over the combined SP14/FA15
dataset.
% We used a learning rate $\eta=0.001$, $L_2$ regularization rate
%$\lambda=0.001$, and mini-batch size of 200.
%
We trained for a single epoch on feature sets (1) and (2), and for 8
epochs on (3), so that the total number of training samples would be
roughly equal for each feature set.
%
\lstDeleteShortInline{|} % sigh...
In addition to accuracy, we report each
classifier's \emph{recall} --- \ie ``How many true changes can we
remember?'' --- defined as
$$
\frac{|\mathsf{predicted} \cap \mathsf{oracle}|}
     {|\mathsf{oracle}|}
$$
where $\mathsf{predicted}$ is limited to the top 3 predictions, and
$\mathsf{oracle}$ is the student's fix, limited to changes that are in
the type error slice.
%
We make the latter distinction as:
%
(1) changes that are not part of the type error slice are noise in the
data set; and
%
(2) it makes the comparison easier to interpret since $\mathsf{oracle}$
never changes.
% NOTE: keep this at the end of the para or it screws up spacing...
\lstMakeShortInline{|}
%
\input{evaluation-utility-graph}
% \begin{table}[ht]
%   \caption{
%     Impact of Type Error Slice on Accuracy.
%     \ES{TODO: load these numbers from CSV}
%   }\label{tab:type-error-slice}
%   \centering
%   \begin{tabular}{lrcrrrrcrrrr}
%     \toprule
%                        &             & & \multicolumn{4}{c} \linear        & & \multicolumn{4}{c} \hiddenFH      \\
%                                          \cmidrule{4-7}                        \cmidrule{9-12}
%     Feature Set        & \# Features & & Top-1  & Top-2  & Top-3  & Recall & & Top-1  & Top-2  & Top-3  & Recall \\
%     \midrule
%     Local Syntax       & 44          & & 27.7\% & 46.7\% & 58.6\% & 38.2\% & & 32.6\% & 49.5\% & 60.2\% & 39.4\% \\
%     + \InSlice         & 45          & & 46.4\% & 65.1\% & 76.1\% & 48.9\% & & 55.4\% & 71.0\% & 82.1\% & 57.1\% \\
%     Filter by \InSlice & 44          & & 55.9\% & 71.9\% & 82.9\% & 57.5\% & & 57.8\% & 72.7\% & 82.9\% & 57.6\% \\
%     \bottomrule
%   \end{tabular}
% \end{table}

\mypara{Results}
\autoref{fig:slice-utility} shows the results of our experiment.
%
As expected, the baseline performs the worst, with a mere 25\% \linear
Top-1 accuracy.
%
Adding \InSlice improves the results substantially with a 45\% \linear Top-1
accuracy, demonstrating the importance of a minimal error slice.
%
However, filtering out expressions that are not part of the slice
\emph{further} improves the results to 54\% \linear Top-1 accuracy.
%
Interestingly, while the \hiddenFH performs similarly poor with no error
slice features, it recovers nearly all of its accuracy after being given
the error slice features.
%
Top-1 accuracy jumps from 29\% to 53\% when we add \InSlice, and only
improves by 1\% when we filter out expressions that are not part of the
error slice.
%
Still, the accuracy gain comes at zero cost, and given the other benefits
of filtering by \InSlice % out expressions that do not belong to the type error slice
--- shrinking the search space and guaranteeing our predictions are actionable ---
we choose to filter all programs by \InSlice.

\mysubsubsection{Contextual Features}
\label{sec:contextual-features}

We investigate the relative impact of the other
three classes of features discussed in \autoref{sec:features}, assuming
we have discarded expressions not in the type error slice.
%
For this experiment we consider again a baseline of only local syntactic
features, extended by each combination of
%
(1) expression size;
(2) contextual syntactic features; and
(3) typing features.
%
As before, we perform a 10-fold cross-validation,
% with $\eta = 0.001$,
% $\lambda = 0.001$, and a mini-batch size of 200
but we train for a full 20 epochs to make the differences more apparent.
%
% \begin{table}[ht]
%   \caption{
%     Impact of Contextual Features on Accuracy.
%     \ES{TODO: load these numbers from CSV}
%   }\label{tab:contextual-features}
%   \centering
%   \begin{tabular}{lrcrrrrcrrrr}
%     \toprule
%                              &             & & \multicolumn{4}{c} \linear        & & \multicolumn{4}{c} \hiddenFH      \\
%                                                \cmidrule{4-7}                        \cmidrule{9-12}
%     Feature Set              & \# Features & & Top-1  & Top-2  & Top-3  & Recall & & Top-1  & Top-2  & Top-3  & Recall \\
%     \midrule
%     Local Syntax             &  44         & & 56.6\% & 72.6\% & 83.2\% & 57.7\% & & 57.6\% & 72.6\% & 83.1\% & 57.7\% \\
%     \midrule
%     + Size                   &  45         & & 57.2\% & 73.7\% & 83.0\% & 57.4\% & & 60.9\% & 75.5\% & 84.1\% & 57.9\% \\
%     + Context                & 220         & & 61.1\% & 78.7\% & 86.7\% & 63.0\% & & 71.5\% & 84.8\% & 90.8\% & 69.0\% \\
%     + Types                  & 102         & & 63.1\% & 77.8\% & 85.2\% & 61.6\% & & 73.4\% & 85.4\% & 90.7\% & 68.9\% \\
%     \midrule
%     + Context + Size         & 221         & & 61.1\% & 78.8\% & 86.3\% & 62.4\% & & 71.5\% & 84.4\% & 90.8\% & 69.0\% \\
%     + Types + Size           & 103         & & 61.9\% & 78.7\% & 85.6\% & 62.0\% & & 73.1\% & 85.2\% & 91.1\% & 69.5\% \\
%     + Context + Types        & 275         & & 63.1\% & 80.9\% & 88.0\% & 65.0\% & & 77.2\% & 88.3\% & 92.5\% & 72.6\% \\
%     \midrule
%     + Context + Types + Size & 276         & & 62.8\% & 80.5\% & 88.1\% & 65.1\% & & 77.3\% & 88.1\% & 92.7\% & 72.7\% \\
%     \bottomrule
%   \end{tabular}
%   % \begin{minipage}{0.49\linewidth}
%   % \centering
%   % \hiddenF
%   % \begin{tabular}{lrrrr}
%   %   \toprule
%   %   Feature Set                 & Top-1  & Top-2  & Top-3  & Recall \\
%   %   \midrule
%   %   Local Syntax                & 56.9\% & 72.2\% & 82.8\% & 57.9\% \\
%   %   \midrule
%   %   + Size                      & 59.7\% & 74.6\% & 83.0\% & 57.4\% \\
%   %   + Context                   & 70.9\% & 83.7\% & 90.4\% & 69.2\% \\
%   %   + Types                     & 72.1\% & 84.1\% & 90.3\% & 69.3\% \\
%   %   \midrule
%   %   + Size + Context            & 69.8\% & 83.5\% & 90.2\% & 68.6\% \\
%   %   + Size + Types              & 72.3\% & 84.6\% & 90.3\% & 69.5\% \\
%   %   + Context + Types           & 75.5\% & 86.4\% & 91.5\% & 71.7\% \\
%   %   \midrule
%   %   + All                       & 75.0\% & 86.8\% & 91.9\% & 72.0\% \\
%   %   \bottomrule
%   % \end{tabular}
%   % \end{minipage}
% \end{table}
%
\mypara{Results}
\autoref{fig:context-utility} summarizes the results of this experiment.
%
The \linear classifier and the \hiddenFH start off
competitive when given only local syntactic features, but the \hiddenFH
quickly outperforms as we add features.

\ExprSize is the weakest feature, improving \linear Top-1
accuracy by less than 1\% and \hiddenFH by only 4\%.
%
In contrast, the contextual syntactic features improve \linear Top-1
accuracy by 5\% (\resp 16\%), and the typing features improve
Top-1 accuracy by 6\% (\resp 18\%).
%
Furthermore, while \ExprSize does provide some benefit when it is the
only additional feature, it does not appear to provide any real increase
in accuracy when added alongside the contextual or typing features.
%
This is likely explained by \emph{feature overlap}, \ie the contextual
features of ``child'' expressions additionally provide some information
about the size of the subtree.

As one might expect, the typing features are more beneficial than the
contextual syntactic features.
%
They improve Top-1 accuracy by an additional 1\% (\resp 3\%), and are much more
compact --- requiring only 55 typing features compared to 180
contextual syntactic features.
%
This aligns with our intuition that types should be a good summary of
the context of an expression.
%
However, typing features do not appear to \emph{subsume} contextual
syntactic features, the \hiddenFH gains an additional 4\% Top-1 accuracy
when both are added.
